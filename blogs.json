{"status":"ok","feed":{"url":"https://medium.com/feed/@carlonuccio","title":"Stories by Carlo Nuccio on Medium","link":"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2","author":"","description":"Stories by Carlo Nuccio on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*KqvdxprU_BHUnbe-IuWcnw.jpeg"},"items":[{"title":"ELT with Airbyte, dbt, Airflow Astronomer-Cosmos and Google BigQuery","pubDate":"2023-07-30 19:37:30","link":"https://towardsdev.com/elt-with-airbyte-dbt-airflow-astronomer-cosmos-and-google-bigquery-d055672de149?source=rss-a9fc4e4ddc60------2","guid":"https://medium.com/p/d055672de149","author":"Carlo Nuccio","thumbnail":"","description":"\n<h3>Orchestrate data ingestion and transformation for BigQuery with Airbyte, dbt, and Airflow Astronomer-Cosmos</h3>\n<p>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#a74b\">Setup Airbyte</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#95a3\">Add a source</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#13c8\">Add a destination</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#7f23\">Setup the connection</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#4d9e\">Create dbt models</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#e73a\">Put all together with Airflow</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#5976\">Run and\u00a0enjoy</a></p>\n<p>In today\u2019s data-driven world, organizations rely heavily on efficient data integration and transformation processes to drive insights and make informed decisions. Extract, Load, Transform (ELT) has emerged as a powerful approach for handling large volumes of data. In this article, we\u2019ll explore how you can streamline your ELT pipelines using three popular open-source tools: Airbyte, dbt, and\u00a0Airflow.</p>\n<p>Understanding <strong>ELT</strong>: Unlike the traditional Extract, Transform, Load (ETL) approach, ELT<strong> flips the order of the transformation step</strong>. It involves extracting data from various sources, loading it into a data warehouse, and performing transformations directly on the loaded data. This approach offers flexibility and scalability, as the transformations can be executed in a distributed environment like a data warehouse.</p>\n<p><strong>Airbyte</strong> is an open-source data integration platform that simplifies the process of collecting and moving data from different sources to a data warehouse.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/290/0*GUC1z3WY_pNPR1vt.png\"></figure><p><strong>dbt </strong>is an open-source command-line tool that allows you to define and execute transformations in SQL. It brings software engineering principles to data transformation, enabling version control, testing, and collaboration.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/0*auU5TUwnki2n-HdL.png\"></figure><p><strong>Airflow</strong> is an open-source platform for programmatically authoring, scheduling, and monitoring workflows.</p>\n<p><strong>Cosmos</strong>: the easiest and most powerful way to integrate <strong>dbt</strong> +\u00a0<strong>Airflow</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*BVAgyznyUQgkZz1U.jpg\"></figure><p>Let\u2019s get into\u00a0it:</p>\n<h3>Setup Airbyte</h3>\n<pre>git clone https://github.com/airbytehq/airbyte.git<br>cd airbyte<br>./run-ab-platform.sh -b</pre>\n<h4>Add a\u00a0source</h4>\n<p>Navigate to the Sources tab on the left bar. Our demo source will pull data from a MySQL DB, which will pull down the information from <a href=\"https://dev.mysql.com/doc/sakila/en/sakila-installation.html\"><strong>Sakila</strong></a> database.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/662/1*SowD3c4Mt7R8S-5rx9I_wQ.png\"></figure><p>The URL is in this\u00a0format:</p>\n<p><a href=\"http://localhost:8000/workspaces/595baf7b-75d8-4b3c-b25f-e153b5357688/connections/4c35165d-6e9a-47db-a8f1-9a0fa26ec48e/status\">http[s]://[host]:[port]/workspaces/&lt;workspace_id&gt;/connections/<strong>&lt;connection_id&gt;</strong>/status</a></p>\n<p>Save the <a href=\"http://localhost:8000/workspaces/595baf7b-75d8-4b3c-b25f-e153b5357688/connections/4c35165d-6e9a-47db-a8f1-9a0fa26ec48e/status\"><strong>connection_id</strong></a></p>\n<h4>Add a destination</h4>\n<p>Navigate to the Destinations tab on the left bar. Our demo source will put data into Google BigQuery. Define the <strong>project id</strong>, the <strong>default dataset id</strong> where to put data, provide <strong>service account key credentials</strong> and specify the <strong>dataset location</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Pqqq0WcxVppVEea1tT7oog.png\"><figcaption>Airbyte\u200a\u2014\u200aBigQuery Destination Connector</figcaption></figure><h4>Setup the connection</h4>\n<p>Navigate to the Connections tab on the left bar. Select an existing source MySQL\u200a\u2014\u200aSakila\u00a0. Select an existing connection BigQuery, Set up connection. You must select the relative stream and choice a sync\u00a0mode.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FHGj3sHwL-YS2EetNiIfMQ.png\"></figure><h3>Create dbt\u00a0models</h3>\n<p>Let\u2019s create dbt project. We\u2019ll use the dbt project structure described <a href=\"https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview\">here</a>:</p>\n<pre>\u251c\u2500\u2500 dbt_project.yml<br>\u2514\u2500\u2500 models<br>    \u251c\u2500\u2500 marts<br>    \u2502       \u251c\u2500\u2500 _models.yml                      <br>    \u2502       \u2514\u2500\u2500 cumulative_revenue.sql<br>    \u2514\u2500\u2500 staging                             <br>            \u251c\u2500\u2500 _models.yml         <br>            \u251c\u2500\u2500 _sources.yaml           <br>            \u2514\u2500\u2500 stg_payment.sql       </pre>\n<pre># models/staging/stg_payment.sql<br><br>SELECT<br>    *<br>FROM<br>    {{ source('sakila', 'payment') }}</pre>\n<pre># models/marts/cumulative_revenue.sql<br><br>{{<br>    config(<br>        materialized='table',<br>    )<br>}}<br><br>SELECT<br>    date(payment_date) as payment_date,<br>    amount,<br>    SUM(amount) OVER (ORDER BY date(payment_date)) as cumulative_amount<br>FROM<br>    {{ ref('stg_payment') }}<br>ORDER BY<br>    payment_date</pre>\n<pre># models/marts/_models.yaml<br><br>version: 2<br><br>models:<br>  - name: cumulative_revenue<br>    description: \"Cumulative revenue from sales\"</pre>\n<pre># models/staging/_models.yaml<br><br>version: 2<br><br>models:<br>  - name: stg_payment<br>    description: \"Staging model consisting of payment events\"<br>    columns:<br>      - name: payment_id<br>        tests:<br>          - unique<br>          - not_null<br>      - name: customer_id<br>        tests:<br>          - not_null</pre>\n<pre># models/staging/_sources.yaml<br><br>version: 2<br><br>sources:<br>  - name: sakila<br>    schema: &lt;dataset_id&gt;<br>    database: &lt;project_id&gt;<br>    loaded_at_field: _airbyte_emitted_at<br>    tables:<br>      - name: payment</pre>\n<h3>Put all together with Airflow Astronomer Cosmos</h3>\n<p>We\u2019ll Astronomer Cosmos to orchestrate our pipeline:</p>\n<pre>cd ~<br>mkdir airflow<br>astro dev init</pre>\n<p>Edit the Dockerfile and the requirements.txt:</p>\n<pre># Dockerfile<br><br>FROM --platform=linux/arm64 quay.io/astronomer/astro-runtime:8.8.0<br><br>RUN python -m venv dbt_venv &amp;&amp; source dbt_venv/bin/activate &amp;&amp; \\<br>    pip install --no-cache-dir dbt-bigquery &amp;&amp; deactivate</pre>\n<pre># requirements.txt<br><br>astronomer-cosmos<br>apache-airflow-providers-airbyte<br>apache-airflow-providers-google</pre>\n<pre>astro dev start</pre>\n<p>You need to create to connections directly on Airflow, one for Airbyte and one for\u00a0GCP:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MPfDHLmpHWI5wy36JvTSsA.png\"></figure><p>Let\u2019s create the final\u00a0dag:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WiDd5wdta-2CAr-OU-mB6w.png\"></figure><pre>#sakila.py<br><br>from cosmos import ProjectConfig, ProfileConfig, ExecutionConfig<br>from cosmos.profiles import GoogleCloudServiceAccountFileProfileMapping<br>from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator<br>from airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor<br>from datetime import datetime<br>from cosmos.airflow.task_group import DbtTaskGroup<br>from airflow import DAG<br>import os<br><br><br>profile_config = ProfileConfig(<br>    profile_name=\"default\",<br>    target_name=\"dev\",<br>    profile_mapping=GoogleCloudServiceAccountFileProfileMapping(<br>        conn_id=\"google_cloud_connection\",<br>        profile_args={<br>            \"project\": \"&lt;project_id&gt;\",<br>            \"dataset\": \"&lt;dataset_id&gt;\",<br>            \"keyfile\": \"&lt;service_account_key_path&gt;\",<br>        },<br>    ),<br>)<br><br>with DAG(dag_id=\"sakila_dag\", start_date=datetime(2023, 07, 30), catchup=False, max_active_runs=1, concurrency=1):<br>    airbyte_trigger = AirbyteTriggerSyncOperator(<br>        task_id=\"airbyte_trigger\",<br>        airbyte_conn_id=\"airbyte_conn_id\",<br>        connection_id=\"&lt;airbyte_connection_id&gt;\",<br>        asynchronous=True,<br>    )<br><br>    airbyte_sensor = AirbyteJobSensor(<br>        task_id=\"airbyte_sensor\",<br>        airbyte_conn_id=\"airbyte_conn_id\",<br>        airbyte_job_id=\"{{ task_instance.xcom_pull(task_ids='airbyte_trigger', key='return_value') }}\",<br>    )<br><br>    dbt_tg = DbtTaskGroup(<br>        project_config=ProjectConfig(<br>            \"/usr/local/airflow/dags/dbt/sakila\",<br>        ),<br>        profile_config=profile_config,<br>        execution_config=ExecutionConfig(<br>            dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",<br>        ),<br>    )<br><br>    airbyte_trigger &gt;&gt; airbyte_sensor &gt;&gt; dbt_tg</pre>\n<p>This is the final structure of the folder\u00a0\u201cdags\u201d:</p>\n<pre>dags<br>\u251c\u2500\u2500 dbt<br>\u2502   \u2514\u2500\u2500 sakila<br>\u2502       \u251c\u2500\u2500 README.md<br>\u2502       \u251c\u2500\u2500 dbt_project.yml<br>\u2502       \u2514\u2500\u2500 models<br>\u2502           \u251c\u2500\u2500 marts<br>\u2502           \u2502   \u251c\u2500\u2500 _models.yml<br>\u2502           \u2502   \u2514\u2500\u2500 cumulative_revenue.sql<br>\u2502           \u2514\u2500\u2500 staging<br>\u2502               \u251c\u2500\u2500 _models.yml<br>\u2502               \u251c\u2500\u2500 _sources.yml<br>\u2502               \u2514\u2500\u2500 stg_payment.sql<br>\u2514\u2500\u2500 sakila_dag<br>    \u2514\u2500\u2500 sakila.py</pre>\n<h3>Run and\u00a0enjoy!</h3>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d055672de149\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdev.com/elt-with-airbyte-dbt-airflow-astronomer-cosmos-and-google-bigquery-d055672de149\">ELT with Airbyte, dbt, Airflow Astronomer-Cosmos and Google BigQuery</a> was originally published in <a href=\"https://towardsdev.com/\">Towards Dev</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Orchestrate data ingestion and transformation for BigQuery with Airbyte, dbt, and Airflow Astronomer-Cosmos</h3>\n<p>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#a74b\">Setup Airbyte</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#95a3\">Add a source</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#13c8\">Add a destination</a><br> \u2218 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#7f23\">Setup the connection</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#4d9e\">Create dbt models</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#e73a\">Put all together with Airflow</a><br>\u00b7 <a href=\"https://medium.com/@carlonuccio?source=rss-a9fc4e4ddc60------2#5976\">Run and\u00a0enjoy</a></p>\n<p>In today\u2019s data-driven world, organizations rely heavily on efficient data integration and transformation processes to drive insights and make informed decisions. Extract, Load, Transform (ELT) has emerged as a powerful approach for handling large volumes of data. In this article, we\u2019ll explore how you can streamline your ELT pipelines using three popular open-source tools: Airbyte, dbt, and\u00a0Airflow.</p>\n<p>Understanding <strong>ELT</strong>: Unlike the traditional Extract, Transform, Load (ETL) approach, ELT<strong> flips the order of the transformation step</strong>. It involves extracting data from various sources, loading it into a data warehouse, and performing transformations directly on the loaded data. This approach offers flexibility and scalability, as the transformations can be executed in a distributed environment like a data warehouse.</p>\n<p><strong>Airbyte</strong> is an open-source data integration platform that simplifies the process of collecting and moving data from different sources to a data warehouse.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/290/0*GUC1z3WY_pNPR1vt.png\"></figure><p><strong>dbt </strong>is an open-source command-line tool that allows you to define and execute transformations in SQL. It brings software engineering principles to data transformation, enabling version control, testing, and collaboration.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/0*auU5TUwnki2n-HdL.png\"></figure><p><strong>Airflow</strong> is an open-source platform for programmatically authoring, scheduling, and monitoring workflows.</p>\n<p><strong>Cosmos</strong>: the easiest and most powerful way to integrate <strong>dbt</strong> +\u00a0<strong>Airflow</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*BVAgyznyUQgkZz1U.jpg\"></figure><p>Let\u2019s get into\u00a0it:</p>\n<h3>Setup Airbyte</h3>\n<pre>git clone https://github.com/airbytehq/airbyte.git<br>cd airbyte<br>./run-ab-platform.sh -b</pre>\n<h4>Add a\u00a0source</h4>\n<p>Navigate to the Sources tab on the left bar. Our demo source will pull data from a MySQL DB, which will pull down the information from <a href=\"https://dev.mysql.com/doc/sakila/en/sakila-installation.html\"><strong>Sakila</strong></a> database.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/662/1*SowD3c4Mt7R8S-5rx9I_wQ.png\"></figure><p>The URL is in this\u00a0format:</p>\n<p><a href=\"http://localhost:8000/workspaces/595baf7b-75d8-4b3c-b25f-e153b5357688/connections/4c35165d-6e9a-47db-a8f1-9a0fa26ec48e/status\">http[s]://[host]:[port]/workspaces/&lt;workspace_id&gt;/connections/<strong>&lt;connection_id&gt;</strong>/status</a></p>\n<p>Save the <a href=\"http://localhost:8000/workspaces/595baf7b-75d8-4b3c-b25f-e153b5357688/connections/4c35165d-6e9a-47db-a8f1-9a0fa26ec48e/status\"><strong>connection_id</strong></a></p>\n<h4>Add a destination</h4>\n<p>Navigate to the Destinations tab on the left bar. Our demo source will put data into Google BigQuery. Define the <strong>project id</strong>, the <strong>default dataset id</strong> where to put data, provide <strong>service account key credentials</strong> and specify the <strong>dataset location</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Pqqq0WcxVppVEea1tT7oog.png\"><figcaption>Airbyte\u200a\u2014\u200aBigQuery Destination Connector</figcaption></figure><h4>Setup the connection</h4>\n<p>Navigate to the Connections tab on the left bar. Select an existing source MySQL\u200a\u2014\u200aSakila\u00a0. Select an existing connection BigQuery, Set up connection. You must select the relative stream and choice a sync\u00a0mode.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FHGj3sHwL-YS2EetNiIfMQ.png\"></figure><h3>Create dbt\u00a0models</h3>\n<p>Let\u2019s create dbt project. We\u2019ll use the dbt project structure described <a href=\"https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview\">here</a>:</p>\n<pre>\u251c\u2500\u2500 dbt_project.yml<br>\u2514\u2500\u2500 models<br>    \u251c\u2500\u2500 marts<br>    \u2502       \u251c\u2500\u2500 _models.yml                      <br>    \u2502       \u2514\u2500\u2500 cumulative_revenue.sql<br>    \u2514\u2500\u2500 staging                             <br>            \u251c\u2500\u2500 _models.yml         <br>            \u251c\u2500\u2500 _sources.yaml           <br>            \u2514\u2500\u2500 stg_payment.sql       </pre>\n<pre># models/staging/stg_payment.sql<br><br>SELECT<br>    *<br>FROM<br>    {{ source('sakila', 'payment') }}</pre>\n<pre># models/marts/cumulative_revenue.sql<br><br>{{<br>    config(<br>        materialized='table',<br>    )<br>}}<br><br>SELECT<br>    date(payment_date) as payment_date,<br>    amount,<br>    SUM(amount) OVER (ORDER BY date(payment_date)) as cumulative_amount<br>FROM<br>    {{ ref('stg_payment') }}<br>ORDER BY<br>    payment_date</pre>\n<pre># models/marts/_models.yaml<br><br>version: 2<br><br>models:<br>  - name: cumulative_revenue<br>    description: \"Cumulative revenue from sales\"</pre>\n<pre># models/staging/_models.yaml<br><br>version: 2<br><br>models:<br>  - name: stg_payment<br>    description: \"Staging model consisting of payment events\"<br>    columns:<br>      - name: payment_id<br>        tests:<br>          - unique<br>          - not_null<br>      - name: customer_id<br>        tests:<br>          - not_null</pre>\n<pre># models/staging/_sources.yaml<br><br>version: 2<br><br>sources:<br>  - name: sakila<br>    schema: &lt;dataset_id&gt;<br>    database: &lt;project_id&gt;<br>    loaded_at_field: _airbyte_emitted_at<br>    tables:<br>      - name: payment</pre>\n<h3>Put all together with Airflow Astronomer Cosmos</h3>\n<p>We\u2019ll Astronomer Cosmos to orchestrate our pipeline:</p>\n<pre>cd ~<br>mkdir airflow<br>astro dev init</pre>\n<p>Edit the Dockerfile and the requirements.txt:</p>\n<pre># Dockerfile<br><br>FROM --platform=linux/arm64 quay.io/astronomer/astro-runtime:8.8.0<br><br>RUN python -m venv dbt_venv &amp;&amp; source dbt_venv/bin/activate &amp;&amp; \\<br>    pip install --no-cache-dir dbt-bigquery &amp;&amp; deactivate</pre>\n<pre># requirements.txt<br><br>astronomer-cosmos<br>apache-airflow-providers-airbyte<br>apache-airflow-providers-google</pre>\n<pre>astro dev start</pre>\n<p>You need to create to connections directly on Airflow, one for Airbyte and one for\u00a0GCP:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MPfDHLmpHWI5wy36JvTSsA.png\"></figure><p>Let\u2019s create the final\u00a0dag:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WiDd5wdta-2CAr-OU-mB6w.png\"></figure><pre>#sakila.py<br><br>from cosmos import ProjectConfig, ProfileConfig, ExecutionConfig<br>from cosmos.profiles import GoogleCloudServiceAccountFileProfileMapping<br>from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator<br>from airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor<br>from datetime import datetime<br>from cosmos.airflow.task_group import DbtTaskGroup<br>from airflow import DAG<br>import os<br><br><br>profile_config = ProfileConfig(<br>    profile_name=\"default\",<br>    target_name=\"dev\",<br>    profile_mapping=GoogleCloudServiceAccountFileProfileMapping(<br>        conn_id=\"google_cloud_connection\",<br>        profile_args={<br>            \"project\": \"&lt;project_id&gt;\",<br>            \"dataset\": \"&lt;dataset_id&gt;\",<br>            \"keyfile\": \"&lt;service_account_key_path&gt;\",<br>        },<br>    ),<br>)<br><br>with DAG(dag_id=\"sakila_dag\", start_date=datetime(2023, 07, 30), catchup=False, max_active_runs=1, concurrency=1):<br>    airbyte_trigger = AirbyteTriggerSyncOperator(<br>        task_id=\"airbyte_trigger\",<br>        airbyte_conn_id=\"airbyte_conn_id\",<br>        connection_id=\"&lt;airbyte_connection_id&gt;\",<br>        asynchronous=True,<br>    )<br><br>    airbyte_sensor = AirbyteJobSensor(<br>        task_id=\"airbyte_sensor\",<br>        airbyte_conn_id=\"airbyte_conn_id\",<br>        airbyte_job_id=\"{{ task_instance.xcom_pull(task_ids='airbyte_trigger', key='return_value') }}\",<br>    )<br><br>    dbt_tg = DbtTaskGroup(<br>        project_config=ProjectConfig(<br>            \"/usr/local/airflow/dags/dbt/sakila\",<br>        ),<br>        profile_config=profile_config,<br>        execution_config=ExecutionConfig(<br>            dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",<br>        ),<br>    )<br><br>    airbyte_trigger &gt;&gt; airbyte_sensor &gt;&gt; dbt_tg</pre>\n<p>This is the final structure of the folder\u00a0\u201cdags\u201d:</p>\n<pre>dags<br>\u251c\u2500\u2500 dbt<br>\u2502   \u2514\u2500\u2500 sakila<br>\u2502       \u251c\u2500\u2500 README.md<br>\u2502       \u251c\u2500\u2500 dbt_project.yml<br>\u2502       \u2514\u2500\u2500 models<br>\u2502           \u251c\u2500\u2500 marts<br>\u2502           \u2502   \u251c\u2500\u2500 _models.yml<br>\u2502           \u2502   \u2514\u2500\u2500 cumulative_revenue.sql<br>\u2502           \u2514\u2500\u2500 staging<br>\u2502               \u251c\u2500\u2500 _models.yml<br>\u2502               \u251c\u2500\u2500 _sources.yml<br>\u2502               \u2514\u2500\u2500 stg_payment.sql<br>\u2514\u2500\u2500 sakila_dag<br>    \u2514\u2500\u2500 sakila.py</pre>\n<h3>Run and\u00a0enjoy!</h3>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d055672de149\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdev.com/elt-with-airbyte-dbt-airflow-astronomer-cosmos-and-google-bigquery-d055672de149\">ELT with Airbyte, dbt, Airflow Astronomer-Cosmos and Google BigQuery</a> was originally published in <a href=\"https://towardsdev.com/\">Towards Dev</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["bigquery","airflow","dbt","modern-data-stack","airbyte"]},{"title":"BigQuery: A guide to managing Cron Expression using UDF","pubDate":"2022-02-23 17:49:48","link":"https://carlonuccio.medium.com/bigquery-a-guide-to-managing-cron-expression-using-udf-b364a5dcdbc8?source=rss-a9fc4e4ddc60------2","guid":"https://medium.com/p/b364a5dcdbc8","author":"Carlo Nuccio","thumbnail":"","description":"\n<h3>Let\u2019s see how to manage cron expression on BigQuery using User-defined functions.</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*8on_ZEh7fEULPveFjNkV9A.png\"></figure><blockquote>User-defined functions (<a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\">UDFs</a>) are a feature of SQL supported by BigQuery that enables a user to create a function using another SQL expression or JavaScript. These functions accept columns of input and perform actions, returning the result of those actions as a\u00a0value.</blockquote>\n<p>Imagine a scenario where you have to check a cron expression with a timestamp value:</p>\n<pre>Cron expression: 0 7 * * 3<br>Timestamp: 2021-11-02 09:57:47.558576 UTC</pre>\n<p>I\u2019ve defined a\u00a0UDF:</p>\n<pre>CREATE OR REPLACE FUNCTION `carlonuccio.CronMatch`(<br>cron STRING, <br>custom_timestamp TIMESTAMP<br>) AS ((<br>  with schedule as (<br>       select  regexp_substr(cron,'[0-9,*]+',1,1) min,<br>               regexp_substr(cron,'[0-9,*]+',1,2) hour,<br>               regexp_substr(cron,'[0-9,*]+',1,3) day,<br>               regexp_substr(cron,'[0-9,*]+',1,4) mon,<br>               regexp_substr(cron,'[0-9,*]+',1,5) wday<br>   )<br>  SELECT IF (<br>     (min  = '*' or  cast(extract(MINUTE from custom_timestamp) as string) in UNNEST(SPLIT(min, ',')))<br>     and (hour  = '*' or  cast(extract(HOUR from custom_timestamp) as string) in UNNEST(SPLIT(hour, ',')))<br>     and (day  = '*' or  cast(extract(DAY from custom_timestamp) as string) in UNNEST(SPLIT(day, ',')))<br>     and (mon  = '*' or  cast(extract(MONTH from custom_timestamp)  as string) in UNNEST(SPLIT(mon, ',')))<br>     and (wday  = '*' or  cast(extract(DAYOFWEEK from custom_timestamp) -1 as string) in UNNEST(SPLIT(wday, ',')))   ,<br>    1,<br>    0<br>  )<br>  FROM schedule<br>));</pre>\n<p><strong>So let\u2019s see in the\u00a0action:</strong></p>\n<pre>SELECT carlonuccio.CronMatch('0 7 * * 3', timestamp('2021-11-02 09:57:47.558576 UTC'))</pre>\n<pre>0</pre>\n<pre>SELECT carlonuccio.CronMatch('0 7 * * 3', timestamp('2021-11-17 07:00:47.558576 UTC'))</pre>\n<pre>1</pre>\n<p><strong>Let\u2019s try multiple schedules:</strong></p>\n<pre>SELECT carlonuccio.CronMatch('0 7 2,17 * 3', timestamp('2021-11-17 07:00:47.558576 UTC'))</pre>\n<pre>1</pre>\n<pre>SELECT carlonuccio.CronMatch('0 7 2,17 * 3', timestamp('2021-11-16 07:00:47.558576 UTC'))</pre>\n<pre>0</pre>\n<p><strong>You can use it to check if a Cron Job has\u00a0run:</strong></p>\n<pre>with cron_table as (<br>  select 1 as job_id, '0 7 * * 3' as cron<br>),<br>job_logs as (<br>  select job_id, timestamp_job<br>  from `&lt;project_id&gt;.&lt;dataset&gt;.job_logs`<br>)<br>select c.job_id,<br>       j.timestamp_job,<br>       carlonuccio.CronMatch(c.cron, timestamp_job)<br>from job_logs j join cron_table c on j.job_id = c.job_id</pre>\n<p><strong>or to calculate next\u00a0runs:</strong></p>\n<pre>with cron_table as (<br>  select 1 as job_id, '0 7 * * 3' as cron<br>),<br>job_logs as (<br>  select job_id, max(timestamp_job) as last_timestamp_job<br>  from `&lt;project_id&gt;.&lt;dataset&gt;.job_logs`<br>  group by job_id<br>),<br>next_minutes as (<br>  SELECT *<br>  FROM UNNEST( GENERATE_TIMESTAMP_ARRAY(TIMESTAMP('2021-01-01 00:00:00'), TIMESTAMP('2022-01-01 00:00:00'), INTERVAL 1 MINUTE)) AS next_timestamp_job<br>)<br>select c.job_id,<br>       next_timestamp_job<br>from next_minutes n, cron_table c join job_logs j on c.job_id = j.job_id<br>where carlonuccio.CronMatch(c.cron, next_timestamp_job) = 1<br>and next_timestamp_job &gt; last_timestamp_job<br>order by 2</pre>\n<p>I hope you have enjoyed this\u00a0UDF.</p>\n<h3>Documentation</h3>\n<ul><li>Implement / and\u00a0-</li></ul>\n<h3>Documentation</h3>\n<ul><li><a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\">https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions</a></li></ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b364a5dcdbc8\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Let\u2019s see how to manage cron expression on BigQuery using User-defined functions.</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*8on_ZEh7fEULPveFjNkV9A.png\"></figure><blockquote>User-defined functions (<a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\">UDFs</a>) are a feature of SQL supported by BigQuery that enables a user to create a function using another SQL expression or JavaScript. These functions accept columns of input and perform actions, returning the result of those actions as a\u00a0value.</blockquote>\n<p>Imagine a scenario where you have to check a cron expression with a timestamp value:</p>\n<pre>Cron expression: 0 7 * * 3<br>Timestamp: 2021-11-02 09:57:47.558576 UTC</pre>\n<p>I\u2019ve defined a\u00a0UDF:</p>\n<pre>CREATE OR REPLACE FUNCTION `carlonuccio.CronMatch`(<br>cron STRING, <br>custom_timestamp TIMESTAMP<br>) AS ((<br>  with schedule as (<br>       select  regexp_substr(cron,'[0-9,*]+',1,1) min,<br>               regexp_substr(cron,'[0-9,*]+',1,2) hour,<br>               regexp_substr(cron,'[0-9,*]+',1,3) day,<br>               regexp_substr(cron,'[0-9,*]+',1,4) mon,<br>               regexp_substr(cron,'[0-9,*]+',1,5) wday<br>   )<br>  SELECT IF (<br>     (min  = '*' or  cast(extract(MINUTE from custom_timestamp) as string) in UNNEST(SPLIT(min, ',')))<br>     and (hour  = '*' or  cast(extract(HOUR from custom_timestamp) as string) in UNNEST(SPLIT(hour, ',')))<br>     and (day  = '*' or  cast(extract(DAY from custom_timestamp) as string) in UNNEST(SPLIT(day, ',')))<br>     and (mon  = '*' or  cast(extract(MONTH from custom_timestamp)  as string) in UNNEST(SPLIT(mon, ',')))<br>     and (wday  = '*' or  cast(extract(DAYOFWEEK from custom_timestamp) -1 as string) in UNNEST(SPLIT(wday, ',')))   ,<br>    1,<br>    0<br>  )<br>  FROM schedule<br>));</pre>\n<p><strong>So let\u2019s see in the\u00a0action:</strong></p>\n<pre>SELECT carlonuccio.CronMatch('0 7 * * 3', timestamp('2021-11-02 09:57:47.558576 UTC'))</pre>\n<pre>0</pre>\n<pre>SELECT carlonuccio.CronMatch('0 7 * * 3', timestamp('2021-11-17 07:00:47.558576 UTC'))</pre>\n<pre>1</pre>\n<p><strong>Let\u2019s try multiple schedules:</strong></p>\n<pre>SELECT carlonuccio.CronMatch('0 7 2,17 * 3', timestamp('2021-11-17 07:00:47.558576 UTC'))</pre>\n<pre>1</pre>\n<pre>SELECT carlonuccio.CronMatch('0 7 2,17 * 3', timestamp('2021-11-16 07:00:47.558576 UTC'))</pre>\n<pre>0</pre>\n<p><strong>You can use it to check if a Cron Job has\u00a0run:</strong></p>\n<pre>with cron_table as (<br>  select 1 as job_id, '0 7 * * 3' as cron<br>),<br>job_logs as (<br>  select job_id, timestamp_job<br>  from `&lt;project_id&gt;.&lt;dataset&gt;.job_logs`<br>)<br>select c.job_id,<br>       j.timestamp_job,<br>       carlonuccio.CronMatch(c.cron, timestamp_job)<br>from job_logs j join cron_table c on j.job_id = c.job_id</pre>\n<p><strong>or to calculate next\u00a0runs:</strong></p>\n<pre>with cron_table as (<br>  select 1 as job_id, '0 7 * * 3' as cron<br>),<br>job_logs as (<br>  select job_id, max(timestamp_job) as last_timestamp_job<br>  from `&lt;project_id&gt;.&lt;dataset&gt;.job_logs`<br>  group by job_id<br>),<br>next_minutes as (<br>  SELECT *<br>  FROM UNNEST( GENERATE_TIMESTAMP_ARRAY(TIMESTAMP('2021-01-01 00:00:00'), TIMESTAMP('2022-01-01 00:00:00'), INTERVAL 1 MINUTE)) AS next_timestamp_job<br>)<br>select c.job_id,<br>       next_timestamp_job<br>from next_minutes n, cron_table c join job_logs j on c.job_id = j.job_id<br>where carlonuccio.CronMatch(c.cron, next_timestamp_job) = 1<br>and next_timestamp_job &gt; last_timestamp_job<br>order by 2</pre>\n<p>I hope you have enjoyed this\u00a0UDF.</p>\n<h3>Documentation</h3>\n<ul><li>Implement / and\u00a0-</li></ul>\n<h3>Documentation</h3>\n<ul><li><a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\">https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions</a></li></ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b364a5dcdbc8\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["function","bigquery","cron","gcp"]}]}